# Clustering and Classification
Today is
```{r echo=FALSE}
date()
```

This week we use the Boston data from the MASS package. The data have 14 variables and 506 observartions for each variable. The variables are either numerical or integers.
```{r}
# Package
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
summary(Boston)
```
Namely, the variables are

- *crim* per capita crime rate by town.
- *zn* proportion of residential land zoned for lots over 25,000 sq.ft.
- *indus* proportion of non-retail business acres per town.
- *chas* Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- *nox* nitrogen oxides concentration (parts per 10 million).
- *rm* average number of rooms per dwelling.
- *age* proportion of owner-occupied units built prior to 1940.
- *boxcox* 21
- *dis* weighted mean of distances to five Boston employment centres.
- *rad* index of accessibility to radial highways.
- *tax* full-value property-tax rate per \$10,000.
- *ptratio* pupil-teacher ratio by town.
- *black* $1000(Bk âˆ’ 0.63)^2$ where Bk is the proportion of blacks by town.
- *lstat* lower status of the population (percent).
- *medv* median value of owner-occupied homes in \$1000s.

The correlation matrix can be illustrated by using *corrplot* package as follows
```{r}
library(corrplot)
corr_matrix<-cor(Boston)
corrplot(corr_matrix)
```
That is, there is a a significant positive correlation between *crim*, *rad*, *tax* and *lsat*. On the other hand, the weighted mean of distances to five Boston employment centres, *dis*, has negative correlation with *indus*, *nox*, and *age*.

However, since the crime rate seems to be the variable in interest, let us illustrate it by some graphics.
```{r}
require(ggplot2)
require(plotly)
plot_ly(data = Boston, x = ~crim, type = "histogram")
plot_ly(data = Boston, x = ~rad, y = ~crim)
plot_ly(data = Boston, x = ~tax, y = ~crim)
plot_ly(data = Boston, x = ~lstat, y = ~crim)
```
Next we standardize the dataset and print out summaries of the scaled data.
```{r}
scaled.Boston <- data.frame(scale(Boston))
```
Now all variables have mean of $0$ and variance $1$. For instance, now the plot of *crim* and *lstat* is the following.
```{r}
plot_ly(data = scaled.Boston, x = ~lstat, y = ~crim)
```
Next we divide *crim* in 5 categories based on the 20/%-quantiles.
```{r}
library(gtools)
q.crim <- quantcut(scaled.Boston$crim,q = 5)
summary(q.crim)
scaled.Boston$crim <- q.crim
```
We divide the dataset into train and test sets, so that 80/% of the data belongs to the train set.
```{r}
sample <- sample.int(n = nrow(scaled.Boston), size = floor(.8*nrow(scaled.Boston)), replace = F)
train <- scaled.Boston[sample, ]
test  <- scaled.Boston[-sample, ]
```
The linear discriminant analysis:
```{r}
# MASS and train are available

# linear discriminant analysis
lda.fit <- lda(crim~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crim)

# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```
Next we cross tabulate the results with the crime categories from the test set.
```{r}
# lda.fit, correct_classes and test are available
correct_classes <- test$crim

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

Let us next reload the Boston dataset and standardize it. Then we calculate the distances between the observations.
```{r}
# load MASS and Boston
library(MASS)
data('Boston')

# standardization
Boston <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

```
One way to determine the number of clusters is to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. The optimal number of clusters is when the value of total WCSS changes radically. In this case, two clusters would seem optimal.
```{r}
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```